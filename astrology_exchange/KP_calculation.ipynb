{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGC3edL9hJds"
      },
      "source": [
        "## Подключение к чистой базе данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udSVlSPvhOkB",
        "outputId": "d64e3fb0-9775-4d7f-9560-248774299466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.2\n",
            "Collecting skyfield\n",
            "  Downloading skyfield-1.49-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from skyfield) (2024.12.14)\n",
            "Collecting jplephem>=2.13 (from skyfield)\n",
            "  Downloading jplephem-2.22-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skyfield) (1.26.4)\n",
            "Collecting sgp4>=2.2 (from skyfield)\n",
            "  Downloading sgp4-2.23-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Downloading skyfield-1.49-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.2/336.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jplephem-2.22-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sgp4-2.23-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.3/232.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sgp4, jplephem, skyfield\n",
            "Successfully installed jplephem-2.22 sgp4-2.23 skyfield-1.49\n"
          ]
        }
      ],
      "source": [
        "!pip install pymysql\n",
        "!pip install pyngrok\n",
        "!pip install skyfield"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7BhtdjXLmcx"
      },
      "source": [
        "## Функциональная часть все все действующие функции для предсказания прогноза цен на акции\n",
        "\n",
        "## Дополнительно сюда встовил запуск базы данных для автоматизации запуска скрипта"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwmljXLpMM4R"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "from sqlalchemy import create_engine\n",
        "from skyfield.api import load\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import json\n",
        "\n",
        "db_config = {\n",
        "    \"host\": \"sql7.freemysqlhosting.net\",\n",
        "    \"user\": \"sql7755157\",\n",
        "    \"password\": \"EpJR55HqCE\",\n",
        "    \"database\": \"sql7755157\",\n",
        "    \"port\": 3306\n",
        "}\n",
        "\n",
        "DB_SERVER = \"sql7.freemysqlhosting.net\"\n",
        "DB_NAME = \"sql7755157\"\n",
        "DB_USER = \"sql7755157\"\n",
        "DB_PASSWORD = \"EpJR55HqCE\"\n",
        "DB_PORT = 3306\n",
        "\n",
        "\n",
        "\n",
        "BASE_URL = \"https://iss.moex.com/iss/history/engines/stock/markets/shares/securities\"\n",
        "\n",
        "def fetch_first_trading_day(company):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(f\"{BASE_URL}/{company}.json\", params={\"iss.meta\": \"off\", \"lang\": \"ru\"})\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        history_data = data.get('history', {}).get('data', [])\n",
        "        history_columns = data.get('history', {}).get('columns', [])\n",
        "\n",
        "        df = pd.DataFrame(history_data, columns=history_columns)\n",
        "\n",
        "        if \"TRADEDATE\" in df.columns:\n",
        "            return df[\"TRADEDATE\"].min()\n",
        "        else:\n",
        "            raise ValueError(\"TRADEDATE колонка не найдена в данных.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Ошибка при запросе первого дня торгов: {e}\")\n",
        "\n",
        "def fetch_historical_data(company, start_date):\n",
        "\n",
        "    all_data = []\n",
        "    start = 0\n",
        "    try:\n",
        "        while True:\n",
        "\n",
        "            params = {\n",
        "                \"from\": start_date,\n",
        "                \"iss.meta\": \"off\",\n",
        "                \"start\": start,\n",
        "                \"lang\": \"ru\"\n",
        "            }\n",
        "\n",
        "            response = requests.get(f\"{BASE_URL}/{company}.json\", params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "            history_data = data.get('history', {}).get('data', [])\n",
        "            history_columns = data.get('history', {}).get('columns', [])\n",
        "\n",
        "            if not history_data:\n",
        "                break\n",
        "\n",
        "            df = pd.DataFrame(history_data, columns=history_columns)\n",
        "            all_data.append(df)\n",
        "\n",
        "            start += 100\n",
        "\n",
        "        full_data = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "        if \"TRADEDATE\" in full_data.columns and \"CLOSE\" in full_data.columns:\n",
        "            result_df = full_data[[\"TRADEDATE\", \"CLOSE\"]].rename(columns={\n",
        "                \"TRADEDATE\": \"date\",\n",
        "                \"CLOSE\": \"value\"\n",
        "            })\n",
        "            return result_df\n",
        "        else:\n",
        "            raise ValueError(\"Не найдены необходимые колонки (TRADEDATE и CLOSE).\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Ошибка при запросе исторических данных: {e}\")\n",
        "\n",
        "def company_stock_history(company):\n",
        "\n",
        "    try:\n",
        "        first_day = fetch_first_trading_day(company)\n",
        "        historical_data = fetch_historical_data(company, first_day)\n",
        "        return historical_data\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка: {e}\")\n",
        "        return None\n",
        "\n",
        "def merge_stock_with_astro(stock_data):\n",
        "\n",
        "    if 'date' not in stock_data.columns:\n",
        "        raise ValueError(\"Датасет 'stock_data' должен содержать столбец 'date'.\")\n",
        "    engine = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_SERVER}:{DB_PORT}/{DB_NAME}\")\n",
        "\n",
        "    start_date = stock_data['date'].min()\n",
        "    end_date = stock_data['date'].max()\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT *,\n",
        "           CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS full_date\n",
        "    FROM astro\n",
        "    WHERE CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) BETWEEN '{start_date}' AND '{end_date}';\n",
        "    \"\"\"\n",
        "\n",
        "    astro_data = pd.read_sql(query, engine)\n",
        "\n",
        "    astro_data['full_date'] = pd.to_datetime(astro_data['full_date'])\n",
        "\n",
        "    stock_data['date'] = pd.to_datetime(stock_data['date'])\n",
        "\n",
        "    eph_stock_data = pd.merge(stock_data, astro_data, left_on='date', right_on='full_date', how='left')\n",
        "\n",
        "    eph_stock_data = eph_stock_data.drop(columns=['full_date'])\n",
        "\n",
        "    return eph_stock_data\n",
        "\n",
        "def prepare_and_train_model(eph_stock_data, target_column='value', timesteps=30, epochs=20, batch_size=32):\n",
        "\n",
        "    eph_stock_data = eph_stock_data.dropna()\n",
        "\n",
        "    eph_stock_data = eph_stock_data.drop(columns=['date_x', 'date_y'], errors='ignore')\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    feature_columns = eph_stock_data.columns.difference([target_column])\n",
        "    eph_stock_data[feature_columns] = scaler.fit_transform(eph_stock_data[feature_columns])\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(eph_stock_data) - timesteps):\n",
        "\n",
        "        X.append(eph_stock_data.iloc[i:i+timesteps][feature_columns].values)\n",
        "\n",
        "        y.append(eph_stock_data.iloc[i+timesteps][target_column])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Создаем RNN модель\n",
        "    input_shape = (X.shape[1], X.shape[2])\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(64, activation='tanh', input_shape=input_shape, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(32, activation='tanh', return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Обучаем модель\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "    callbacks = [early_stopping, model_checkpoint]\n",
        "\n",
        "    model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=callbacks, verbose=1)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_stock_predict(trained_model, stock_data, target_column='value', timesteps=30):\n",
        "\n",
        "    engine = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_SERVER}:{DB_PORT}/{DB_NAME}\")\n",
        "\n",
        "\n",
        "    moscow_timezone = pytz.timezone(\"Europe/Moscow\")\n",
        "    current_time = datetime.now(moscow_timezone)\n",
        "\n",
        "    start_date = stock_data['date'].max() + pd.Timedelta(\"1 day\")\n",
        "    end_date = stock_data['date'].max() + pd.Timedelta(\"31 day\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT *,\n",
        "           CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS full_date\n",
        "    FROM astro\n",
        "    WHERE CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) BETWEEN '{start_date}' AND '{end_date}';\n",
        "    \"\"\"\n",
        "\n",
        "    astro_data = pd.read_sql(query, engine)\n",
        "\n",
        "    astro_data['date'] = pd.to_datetime(astro_data[['year', 'month', 'day']])\n",
        "\n",
        "    stock_data['date'] = pd.to_datetime(stock_data['date'])\n",
        "    merged_data = pd.merge(stock_data, astro_data, on='date', how='right')\n",
        "\n",
        "    merged_data = merged_data.drop(columns=['date', 'full_date','value'], errors='ignore')\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    feature_columns = merged_data.columns.difference([target_column])\n",
        "\n",
        "    merged_data[feature_columns] = scaler.fit_transform(merged_data[feature_columns])\n",
        "\n",
        "    initial_window = merged_data.iloc[-timesteps:][feature_columns].values\n",
        "    X_pred = np.array([initial_window])\n",
        "\n",
        "    future_dates = [start_date + pd.Timedelta(f\"{i} day\") for i in range(timesteps)]\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(30):\n",
        "\n",
        "        pred_value = trained_model.predict(X_pred)[0][0]\n",
        "\n",
        "        predictions.append(pred_value)\n",
        "\n",
        "        new_row = np.zeros((1, X_pred.shape[2]))\n",
        "        X_pred = np.concatenate([X_pred[:, 1:, :], new_row[np.newaxis, :, :]], axis=1)\n",
        "        X_pred[0, -1, :] = np.append(X_pred[0, -1, :-1], pred_value)\n",
        "\n",
        "    pred_value = stock_data['value'].iloc[-1]\n",
        "\n",
        "    pred_list = []\n",
        "\n",
        "    coeff = stock_data.dropna(subset=['value']).reset_index(drop=True)\n",
        "\n",
        "    counter = 50 * 4**((-9) * len(coeff) * 10**(-4) +0.3)+0.15\n",
        "\n",
        "    for i in range(timesteps):\n",
        "      pred_value += predictions[i] * counter\n",
        "      pred_list.append(pred_value)\n",
        "\n",
        "    prediction_df = pd.DataFrame({\n",
        "        'date': future_dates,\n",
        "        'predicted_value': pred_list\n",
        "    })\n",
        "\n",
        "    return prediction_df\n",
        "\n",
        "def calculate(tiker):\n",
        "\n",
        "    stock_data = company_stock_history(tiker)\n",
        "    eph_stock_data = merge_stock_with_astro(stock_data)\n",
        "    trained_model = prepare_and_train_model(eph_stock_data)\n",
        "    predicted_prices = get_stock_predict(trained_model, stock_data)\n",
        "\n",
        "    return stock_data, predicted_prices\n",
        "\n",
        "def convert_dates_to_strings(df):\n",
        "\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = df['date'].astype(str)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE4uVK2CAhAh"
      },
      "source": [
        "## Создание датафрейма для обучения модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbtVgCOUiDTR"
      },
      "source": [
        "# Запуск NGROK для связывания гугл коллабов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ftYvBIqTYUZ",
        "outputId": "7b944759-1b68-4beb-d0bc-da734d4186bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Public URL: NgrokTunnel: \"https://3cc7-34-56-131-214.ngrok-free.app\" -> \"http://localhost:8071\"\n",
            "Server is running...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-7994a854c919>:87: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  full_data = pd.concat(all_data, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 12174917.0000 - mae: 2840.3560 - val_loss: 44507532.0000 - val_mae: 6628.2974\n",
            "Epoch 2/20\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 11854480.0000 - mae: 2807.8191 - val_loss: 44548220.0000 - val_mae: 6631.3765\n",
            "Epoch 3/20\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 12013854.0000 - mae: 2809.3313 - val_loss: 44584980.0000 - val_mae: 6634.1592\n",
            "Epoch 4/20\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 12243348.0000 - mae: 2855.1084 - val_loss: 44620236.0000 - val_mae: 6636.8252\n",
            "Epoch 5/20\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 12285563.0000 - mae: 2828.1836 - val_loss: 44654696.0000 - val_mae: 6639.4307\n",
            "Epoch 6/20\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 12526087.0000 - mae: 2848.3335 - val_loss: 44688736.0000 - val_mae: 6642.0039\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jan/2025 18:28:11] \"GET /PHOR HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Обработчик HTTP-запросов\n",
        "!ngrok authtoken '2qaRAbcjuH5F6DqNpVIMLx2Nmj3_2MEDzKSDeWR4s39nrX6ZE'\n",
        "\n",
        "import json\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class SimpleHandler(BaseHTTPRequestHandler):\n",
        "    def do_GET(self):\n",
        "        try:\n",
        "            # Проверка на наличие тикера\n",
        "            ticker = self.path[1:]\n",
        "            if not ticker:\n",
        "                self.send_response(400)\n",
        "                self.end_headers()\n",
        "                self.wfile.write(b\"Ticker is required in the URL path.\")\n",
        "                logging.error(\"Тикер отсутствует в запросе.\")\n",
        "                return\n",
        "\n",
        "            logging.info(f\"Получен запрос для тикера: {ticker}\")\n",
        "\n",
        "            df1, df2 = calculate(ticker)\n",
        "\n",
        "            df1 = convert_dates_to_strings(df1)\n",
        "            df2 = convert_dates_to_strings(df2)\n",
        "\n",
        "            response_data = {\n",
        "                'df1': df1.to_dict(orient='records'),\n",
        "                'df2': df2.to_dict(orient='records')\n",
        "            }\n",
        "\n",
        "            response_json = json.dumps(response_data)\n",
        "            self.send_response(200)\n",
        "            self.send_header('Content-Type', 'application/json')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(response_json.encode())\n",
        "            logging.info(f\"Ответ отправлен: {response_json}\")\n",
        "        except Exception as e:\n",
        "\n",
        "            logging.error(f\"Ошибка обработки запроса: {e}\")\n",
        "            self.send_response(500)\n",
        "            self.end_headers()\n",
        "            self.wfile.write(f\"Server error: {str(e)}\".encode())\n",
        "\n",
        "public_url = ngrok.connect(8071)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "server_address = ('', 8071)\n",
        "httpd = HTTPServer(server_address, SimpleHandler)\n",
        "print(\"Server is running...\")\n",
        "httpd.serve_forever()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}